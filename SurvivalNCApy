#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sun Aug 27 01:56:26 2017

@author: mohamed

Survival NCA (Neighborhood Component Analysis)
"""

# Append relevant paths
import os
import sys

def conditionalAppend(Dir):
    """ Append dir to sys path"""
    if Dir not in sys.path:
        sys.path.append(Dir)

cwd = os.getcwd()
conditionalAppend(cwd)

import numpy as np
import SurvivalUtils as sUtils
import tensorflow as tf


#%%============================================================================
# ---- J U N K ----------------------------------------------------------------
#==============================================================================

from scipy.io import loadmat

# Load data
dpath = "/home/mohamed/Desktop/CooperLab_Research/KNN_Survival/Data/SingleCancerDatasets/GBMLGG/Brain_Integ.mat"
Data = loadmat(dpath)

data = np.float32(Data['Integ_X'])
if np.min(Data['Survival']) < 0:
    Data['Survival'] = Data['Survival'] - np.min(Data['Survival']) + 1

Survival = np.int32(Data['Survival'])
Censored = np.int32(Data['Censored'])

# Get split indices
#splitIdxs = sUtils.getSplitIdxs(data)

# Generate survival status - discretized into months
aliveStatus = sUtils.getAliveStatus(Survival, Censored, scale = 30)


#%%============================================================================
# --- P R O T O T Y P E S -----------------------------------------------------
#==============================================================================





#%%============================================================================
# Building the compuational graph
#==============================================================================

#
# Some parts were Modified from: 
# https://all-umass.github.io/metric-learn/_modules/metric_learn/nca.html#NCA
#

# -----------------------------------------------------------------------------
# Basic ground work
# -----------------------------------------------------------------------------

tf.reset_default_graph()

# Get dims
N, D = np.int32(data.shape)
T = np.int32(aliveStatus.shape[1]) # no of time points

# Graph input
X = tf.placeholder("float32", [N, D], name='X')
alive = tf.placeholder("int32", [N, T], name='alive')

# Get mask of available survival status at different time points
avail_mask = tf.cast((aliveStatus >= 0), tf.int32, name='avail_mask')

# Initialize A to a scaling matrix
A = np.zeros((D, D))
np.fill_diagonal(A, 1./(data.max(axis=0)-data.min(axis=0)))
A = np.float32(A)
A = tf.Variable(A, name='A')

# initilize A and transform input
AX = tf.matmul(X, A)  # shape (N, D)


## -----------------------------------------------------------------------------
## Define core functions
## -----------------------------------------------------------------------------

#def add_Pi_to_cumSum(t, i, cumSum):
#    
#    """ 
#    Gets "probability" of patient i's survival status being correctly 
#    predicted at time t and adds it to running cumSum for all points 
#    whose survival status is known at time t
#    """
#
#    # Get ignore mask -> 
#    # give central point and unknown status zero weight
#    ignoreMask = tf.Variable(avail_mask[:, t]) # unavailable labels at time t are -> 0
#    ignoreMask = ignoreMask[i].assign(0) # central point itself -> 0
#    ignoreMask = tf.cast(ignoreMask, tf.float32)
#    
#    # Calculate normalized feature similarity metric between central point 
#    # and those cases with available survival status at time t
#    softmax = tf.exp(tf.reduce_sum((AX[i,:] - AX)**2, axis=1)) # shape (n)
#    softmax = tf.multiply(softmax, ignoreMask)
#    softmax = softmax / tf.reduce_sum(softmax)
#    
#    # Get label match mask (i.e. i is alive and j is alive, same if i is dead)
#    # note there is no need to set central/censored points to zero since this will
#    # be multiplied by softmax which already has zeros at these locations
#    match = tf.cast(tf.equal(alive[:, t], alive[i, t]), tf.float32)
#    
#    # Get "probability" of correctly classifying i at time t
#    Pi = tf.reduce_sum(tf.multiply(softmax, match))
#    
#    return cumSum + Pi


#def conditional_add_to_cumSum(t, i, cumSum):
#    
#        """ 
#        Add Pi to cumsum if survival status of i is known at time t 
#        """
#        
#        cumSum = tf.cond(tf.equal(avail_mask[i, t], 1), lambda: add_Pi_to_cumSum(t, i, cumSum), lambda: tf.cast(cumSum, tf.float32))
#        
#        
#        # DEBUG !!!! **********************************************************
#        #tf.Print(t, [t], message="t = ")
#        #tf.Print(i, [i], message="i = ")
#        # *********************************************************************
#        
#        # increment t if last patient
#        t = tf.cond(tf.equal(i, N-1), lambda: tf.add(t, 1), lambda: tf.add(t, 0))
#        
#        # increment i (or reset it if last patient)
#        i = tf.cond(tf.equal(i, N-1), lambda: tf.multiply(i, 0), lambda: tf.add(i, 1))
#        
#        return t, i, cumSum

# -----------------------------------------------------------------------------
# Now go through all time points and patients
# -----------------------------------------------------------------------------

#cumSum = tf.cast(tf.Variable([0.0]), tf.float32)
#t = tf.cast(tf.constant(0), tf.int32)
#i = tf.cast(tf.constant(0), tf.int32)

## Doing the following admittedly odd step because tensorflow's loop
## requires both the condition and body to have same number of inputs
#def compare_t(t, i, cumSum):
#    """Check that t < T"""
#    return tf.less(t, T) 
#
## main loop
#cond = lambda t, i, cumSum: compare_t(t, i, cumSum)
#bod = lambda t, i, cumSum: conditional_add_to_cumSum(t, i, cumSum)
#cumSum_ = tf.while_loop(cond, bod, [t, i, cumSum])


# -----------------------------------------------------------------------------
# DEBUG !!! ***************************************************************
# -----------------------------------------------------------------------------


cumSum = tf.cast(tf.Variable([0.0]), tf.float32, name='cumSum')
t = tf.cast(tf.constant(10), tf.int32, name='t')
i = tf.cast(tf.constant(20), tf.int32, name='i')

#cumSum = add_Pi_to_cumSum(t, i, cumSum)

# Get ignore mask -> 
# give central point and unknown status zero weight
ignoreMask = tf.Variable(avail_mask[:, t]) # unavailable labels at time t are -> 0
ignoreMask = ignoreMask[i].assign(0) # central point itself -> 0
ignoreMask = tf.cast(ignoreMask, tf.float32)

# Calculate normalized feature similarity metric between central point 
# and those cases with available survival status at time t
softmax = tf.exp(tf.reduce_sum((AX[i,:] - AX)**2, axis=1)) # shape (n)
#softmax = tf.multiply(softmax, ignoreMask)
#softmax = softmax / tf.reduce_sum(softmax)

# Get label match mask (i.e. i is alive and j is alive, same if i is dead)
# note there is no need to set central/censored points to zero since this will
# be multiplied by softmax which already has zeros at these locations
match = tf.cast(tf.equal(alive[:, t], alive[i, t]), tf.float32)

# Get "probability" of correctly classifying i at time t
Pi = tf.reduce_sum(tf.multiply(softmax, match))

# cusum
cumSum = cumSum + Pi



# -----------------------------------------------------------------------------


#%%============================================================================
# Launch graph
#==============================================================================


with tf.Session() as sess:
    
    init = tf.global_variables_initializer()
    sess.run(init)
    
    feed = {X: data, alive: aliveStatus}
    
    fetches = [t, i, ignoreMask, softmax, match, Pi, cumSum]
    fetch_names = ['t', 'i', 'ignoreMask', 'softmax', 'match', 'Pi', 'cumSum']
        
    f = sess.run(fetches, feed_dict = feed)
    
    fetched = {}
    for i,j in enumerate(f):
        fetched[fetch_names[i]] = j

    
